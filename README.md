# Pipeline de Dados Meta Graph API

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7.3-017CEE?style=flat&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=flat&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![SQL Server](https://img.shields.io/badge/SQL%20Server-CC2927?style=flat&logo=microsoft-sql-server&logoColor=white)](https://www.microsoft.com/sql-server)

Pipeline de dados escalÃ¡vel e pronto para produÃ§Ã£o usando Apache Airflow para extrair, transformar e carregar dados de publicidade da Meta Graph API (Facebook & Instagram) atravÃ©s de mÃºltiplas contas do Business Manager.

## ğŸ¯ VisÃ£o Geral

Este projeto demonstra uma soluÃ§Ã£o robusta de engenharia de dados para gerenciar dados de publicidade de mÃºltiplas contas do Meta Business Manager. Apresenta:

- **OrquestraÃ§Ã£o multi-conta** - Gerencia 10+ contas publicitÃ¡rias com rotaÃ§Ã£o inteligente de tokens
- **Processamento paralelo** - Task groups para performance otimizada de extraÃ§Ã£o
- **Arquitetura enterprise** - SeparaÃ§Ã£o de responsabilidades, gerenciamento de configuraÃ§Ã£o e tratamento de erros
- **SincronizaÃ§Ã£o de banco de dados** - EstratÃ©gia dual-database (PostgreSQL para data lake, SQL Server para analytics)
- **Boas prÃ¡ticas de produÃ§Ã£o** - ConfiguraÃ§Ã£o baseada em variÃ¡veis de ambiente, logging completo, mecanismos de retry

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Apache Airflow DAG                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ Task Group 1 â”‚      â”‚ Task Group 2 â”‚                        â”‚
â”‚  â”‚ Contas 1-5   â”‚  â†’   â”‚ Contas 6-10  â”‚   â†’   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  (Paralelo)  â”‚      â”‚  (Paralelo)  â”‚       â”‚ Sync SQL    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“                                        â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   PostgreSQL     â”‚                    â”‚   SQL Server     â”‚
      â”‚   (Data Lake)    â”‚    â•â•â•â•â•â•â•>        â”‚   (Analytics)    â”‚
      â”‚                  â”‚    Sync Views      â”‚                  â”‚
      â”‚ â€¢ Dados brutos   â”‚                    â”‚ â€¢ Dados agregadosâ”‚
      â”‚ â€¢ Dados actions  â”‚                    â”‚ â€¢ Views negÃ³cio  â”‚
      â”‚ â€¢ Multi-contas   â”‚                    â”‚ â€¢ RelatÃ³rios     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais

1. **Orquestrador DAG** ([meta_graph_api_pipeline.py](dags/meta_graph_api_pipeline.py))
   - Agenda e coordena todas as tarefas
   - Gerencia execuÃ§Ã£o paralela com task groups
   - Lida com retries e cenÃ¡rios de falha

2. **Cliente Graph API** ([utils/graph_api.py](utils/graph_api.py))
   - Abstrai interaÃ§Ãµes com Meta Graph API
   - Implementa paginaÃ§Ã£o e rate limiting
   - Processa e transforma respostas da API

3. **Gerenciador de Banco de Dados** ([utils/database.py](utils/database.py))
   - Gerencia todas as operaÃ§Ãµes de banco de dados
   - Implementa padrÃ£o upsert para consistÃªncia de dados
   - Gerencia sincronizaÃ§Ã£o cross-database

4. **Camada de ConfiguraÃ§Ã£o** ([config/accounts_config.py](config/accounts_config.py))
   - ConfiguraÃ§Ã£o baseada em variÃ¡veis de ambiente
   - Gerenciamento multi-conta com rotaÃ§Ã£o de tokens
   - ValidaÃ§Ã£o e verificaÃ§Ã£o de erros

## ğŸš€ Funcionalidades

### Gerenciamento Multi-Conta
- **ConfiguraÃ§Ã£o dinÃ¢mica de contas** via variÃ¡veis de ambiente
- **RotaÃ§Ã£o inteligente de tokens** para distribuir limites de rate da API
- **Processamento paralelo** com task groups configurÃ¡veis
- **Tabelas por conta** para isolamento e escalabilidade de dados

### Pipeline de Dados Robusto
- **Cargas incrementais** com perÃ­odo de retenÃ§Ã£o configurÃ¡vel (padrÃ£o: 15 dias)
- **OperaÃ§Ãµes upsert** para prevenir duplicatas
- **Tratamento de erros abrangente** com retries automÃ¡ticos
- **Gerenciamento de rate limit** com backoff exponencial

### Recursos Enterprise
- **ConfiguraÃ§Ã£o baseada em ambiente** - Sem credenciais hardcoded
- **Arquitetura modular** - SeparaÃ§Ã£o clara de responsabilidades
- **Logging abrangente** - Visibilidade completa da execuÃ§Ã£o do pipeline
- **SincronizaÃ§Ã£o de banco de dados** - PropagaÃ§Ã£o automÃ¡tica de dados
- **Design escalÃ¡vel** - FÃ¡cil adicionar novas contas ou fontes de dados

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Apache Airflow 2.7.3+
- PostgreSQL 12+
- SQL Server 2019+ (ou Azure SQL Database)
- Conta(s) Meta Business Manager com acesso Ã  API

## ğŸ› ï¸ InstalaÃ§Ã£o

### 1. Clonar o RepositÃ³rio

```bash
git clone https://github.com/seu-usuario/meta-ads-data-pipeline.git
cd meta-ads-data-pipeline
```

### 2. Criar Ambiente Virtual

```bash
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac
```

### 3. Instalar DependÃªncias

```bash
pip install -r requirements.txt
```

### 4. Configurar VariÃ¡veis de Ambiente

Copie o arquivo de exemplo e configure com suas credenciais:

```bash
cp .env.example .env
```

Edite o `.env` com suas configuraÃ§Ãµes:

```bash
# ConfiguraÃ§Ã£o PostgreSQL
POSTGRES_HOST=seu-host-postgres
POSTGRES_PORT=5432
POSTGRES_USER=seu-usuario
POSTGRES_PASSWORD=sua-senha
POSTGRES_DATABASE=seu-database
POSTGRES_SCHEMA=seu-schema

# ConfiguraÃ§Ã£o SQL Server
SQLSERVER_HOST=seu-sqlserver.database.windows.net
SQLSERVER_PORT=1433
SQLSERVER_DATABASE=seu-database
SQLSERVER_USER=seu-usuario
SQLSERVER_PASSWORD=sua-senha
SQLSERVER_SCHEMA=graph

# ConfiguraÃ§Ã£o Meta Graph API
GRAPH_API_TOKENS=token1,token2,token3
META_ACCOUNTS=id_conta_1:bm_01,id_conta_2:bm_02,id_conta_3:bm_03

# ConfiguraÃ§Ã£o da API
GRAPH_API_VERSION=v19.0
DATA_RETENTION_DAYS=15
```

### 5. Configurar Banco de Dados

Crie as tabelas necessÃ¡rias no PostgreSQL:

```sql
-- Criar schema
CREATE SCHEMA IF NOT EXISTS seu_schema;

-- Criar tabela de exemplo para ads
CREATE TABLE seu_schema.bm_01 (
    unique_id VARCHAR(32) PRIMARY KEY,
    account_id VARCHAR(50),
    account_name VARCHAR(255),
    campaign_id VARCHAR(50),
    campaign_name VARCHAR(255),
    campaign_status VARCHAR(50),
    adset_id VARCHAR(50),
    adset_name VARCHAR(255),
    ad_id VARCHAR(50),
    ad_name VARCHAR(255),
    objective VARCHAR(100),
    spend DECIMAL(10, 2),
    clicks INTEGER,
    inline_link_clicks INTEGER,
    impressions INTEGER,
    date DATE
);

-- Criar tabela de actions
CREATE TABLE seu_schema.bm_01_actions (
    account_id VARCHAR(50),
    ad_id VARCHAR(50),
    action_type VARCHAR(100),
    value INTEGER,
    date DATE
);

-- Criar views para consolidaÃ§Ã£o de dados
CREATE VIEW seu_schema.vw_graph_ads AS
SELECT * FROM seu_schema.bm_01
UNION ALL
SELECT * FROM seu_schema.bm_02
-- ... adicione todas as suas tabelas de contas
;
```

### 6. Inicializar Airflow

```bash
# Inicializar banco de dados do Airflow
airflow db init

# Criar usuÃ¡rio admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

## ğŸ® Uso

### Iniciar Airflow

```bash
# Iniciar web server (porta padrÃ£o 8080)
airflow webserver --port 8080

# Em outro terminal, iniciar o scheduler
airflow scheduler
```

### Acessar Interface do Airflow

Navegue para `http://localhost:8080` e faÃ§a login com suas credenciais.

### Habilitar a DAG

1. Encontre a DAG chamada `meta_graph_api_pipeline`
2. Alterne para "On"
3. A DAG executarÃ¡ conforme agendamento: **8:00, 14:00, 20:00 (Seg-SÃ¡b)**

### Trigger Manual

VocÃª pode disparar manualmente a DAG pela UI ou CLI:

```bash
airflow dags trigger meta_graph_api_pipeline
```

## ğŸ“Š Fluxo de Dados

### Fase de ExtraÃ§Ã£o
1. DAG dispara task groups paralelos
2. Cada task busca dados de uma conta via Graph API
3. Dados incluem:
   - MÃ©tricas de performance de anÃºncios (gasto, cliques, impressÃµes)
   - InformaÃ§Ãµes e status de campanhas
   - AÃ§Ãµes e eventos de conversÃ£o
4. Dados sÃ£o validados e transformados

### Fase de Carregamento
1. Dados sÃ£o inseridos no PostgreSQL em tabelas especÃ­ficas por conta
2. Duplicatas sÃ£o prevenidas usando hash unique_id
3. Dados histÃ³ricos mantidos baseado na polÃ­tica de retenÃ§Ã£o

### Fase de SincronizaÃ§Ã£o
1. Views do PostgreSQL agregam dados de todas as contas
2. Dados sÃ£o sincronizados para SQL Server para analytics
3. Registros antigos sÃ£o deletados antes de inserir novos
4. Threads paralelas otimizam transferÃªncias de grandes volumes

## ğŸ”§ ConfiguraÃ§Ã£o

### Adicionar Novas Contas

Simplesmente atualize seu arquivo `.env`:

```bash
META_ACCOUNTS=contas_existentes,nova_conta_id:bm_11
GRAPH_API_TOKENS=tokens_existentes,novo_token
```

O pipeline descobre e processa automaticamente as novas contas.

### Ajustar Agendamento

Modifique o `SCHEDULE_INTERVAL` em [meta_graph_api_pipeline.py](dags/meta_graph_api_pipeline.py):

```python
SCHEDULE_INTERVAL = "0 8,14,20 * * 1-6"  # Formato cron
```

### Customizar RetenÃ§Ã£o de Dados

Atualize o `.env`:

```bash
DATA_RETENTION_DAYS=30  # Buscar Ãºltimos 30 dias ao invÃ©s de 15
```

## ğŸ† Boas PrÃ¡ticas Demonstradas

### OrganizaÃ§Ã£o de CÃ³digo
- âœ… **Design modular** - MÃ³dulos separados para API, database e configuraÃ§Ã£o
- âœ… **PrincÃ­pio DRY** - FunÃ§Ãµes e classes reutilizÃ¡veis
- âœ… **Nomenclatura clara** - CÃ³digo auto-documentado com nomes descritivos

### Gerenciamento de ConfiguraÃ§Ã£o
- âœ… **VariÃ¡veis de ambiente** - Sem credenciais hardcoded
- âœ… **`.env.example`** - Template para fÃ¡cil configuraÃ§Ã£o
- âœ… **ValidaÃ§Ã£o** - VerificaÃ§Ãµes de configuraÃ§Ã£o antes da execuÃ§Ã£o

### Tratamento de Erros
- âœ… **LÃ³gica de retry** - Retries automÃ¡ticos com backoff exponencial
- âœ… **Rate limiting** - Respeita limites da API
- âœ… **Logging abrangente** - Visibilidade completa da execuÃ§Ã£o
- âœ… **DegradaÃ§Ã£o gradual** - Continua processando outras contas em caso de falha

### OperaÃ§Ãµes de Banco de Dados
- âœ… **PadrÃ£o upsert** - Previne duplicatas
- âœ… **Processamento em lote** - InserÃ§Ãµes bulk eficientes
- âœ… **Gerenciamento de transaÃ§Ãµes** - ConsistÃªncia de dados
- âœ… **Connection pooling** - Uso otimizado de recursos

### Pronto para ProduÃ§Ã£o
- âœ… **Type hints** - Melhor suporte de IDE e documentaÃ§Ã£o
- âœ… **Docstrings** - DocumentaÃ§Ã£o clara de funÃ§Ãµes
- âœ… **Logging** - Visibilidade de execuÃ§Ã£o
- âœ… **Estrutura de testes** - Pronto para testes unitÃ¡rios

## ğŸ“ Estrutura do Projeto

```
meta-ads-data-pipeline/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ meta_graph_api_pipeline.py      # DefiniÃ§Ã£o principal da DAG
â”‚   â””â”€â”€ grax_midia_facebook_graph_api_new.py  # Legacy (referÃªncia)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py                      # OperaÃ§Ãµes de banco de dados
â”‚   â””â”€â”€ graph_api.py                     # Cliente Graph API
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ accounts_config.py               # Gerenciamento de contas
â”œâ”€â”€ tests/                               # Testes unitÃ¡rios (a adicionar)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md                  # Arquitetura detalhada
â”‚   â””â”€â”€ GIT_WORKFLOW.md                  # Guia de workflow Git
â”œâ”€â”€ .env.example                         # Template de ambiente
â”œâ”€â”€ .gitignore                           # Regras de ignore do Git
â”œâ”€â”€ requirements.txt                     # DependÃªncias Python
â””â”€â”€ README.md                            # Este arquivo
```

## ğŸ¤ Contribuindo

Este Ã© um projeto de portfÃ³lio, mas sugestÃµes sÃ£o bem-vindas! Consulte [GIT_WORKFLOW.md](docs/GIT_WORKFLOW.md) para diretrizes de contribuiÃ§Ã£o.

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [**Detalhes da Arquitetura**](docs/ARCHITECTURE.md) - Arquitetura tÃ©cnica aprofundada
- [**Workflow Git**](docs/GIT_WORKFLOW.md) - EstratÃ©gia de branches e diretrizes de commit
- [**Guia de Setup**](SETUP_GUIDE.md) - InstruÃ§Ãµes detalhadas de instalaÃ§Ã£o

## ğŸ“ LicenÃ§a

Este projeto Ã© para fins de demonstraÃ§Ã£o de portfÃ³lio.

## ğŸ‘¤ Autor

**Projeto de PortfÃ³lio - Engenharia de Dados**

Demonstrando expertise em:
- OrquestraÃ§Ã£o com Apache Airflow
- IntegraÃ§Ã£o de APIs e extraÃ§Ã£o de dados
- Arquitetura multi-database
- Desenvolvimento Python pronto para produÃ§Ã£o
- Boas prÃ¡ticas de engenharia de dados

### CompetÃªncias TÃ©cnicas Demonstradas

- âœ… **Apache Airflow** - Design de DAGs, agendamento, orquestraÃ§Ã£o
- âœ… **Python AvanÃ§ado** - OOP, type hints, clean code, princÃ­pios SOLID
- âœ… **IntegraÃ§Ã£o de APIs** - Meta Graph API, paginaÃ§Ã£o, rate limiting
- âœ… **Engenharia de Dados** - ETL, transformaÃ§Ã£o, sincronizaÃ§Ã£o
- âœ… **Arquitetura de Dados** - Data lake, analytics layer, multi-database
- âœ… **Boas PrÃ¡ticas** - DocumentaÃ§Ã£o, logging, tratamento de erros
- âœ… **DevOps** - Git workflow, Docker, gerenciamento de configuraÃ§Ã£o
- âœ… **SeguranÃ§a** - GestÃ£o de credenciais, variÃ¡veis de ambiente
- âœ… **Performance** - Processamento paralelo, operaÃ§Ãµes em lote

---

## ğŸ¯ Sobre Este Projeto

Este pipeline resolve um problema real de engenharia de dados: **como gerenciar e processar dados de mÃºltiplas contas publicitÃ¡rias de forma escalÃ¡vel, eficiente e confiÃ¡vel**.

### Problema Resolvido

Empresas que gerenciam mÃºltiplas contas do Meta Business Manager enfrentam desafios como:
- Coleta manual de dados de mÃºltiplas contas
- Rate limits da API
- InconsistÃªncia de dados
- Falta de histÃ³rico consolidado
- Processos nÃ£o escalÃ¡veis

### SoluÃ§Ã£o Implementada

Este pipeline automatiza completamente o processo, oferecendo:
- âœ… ExtraÃ§Ã£o automÃ¡tica de 10+ contas simultaneamente
- âœ… RotaÃ§Ã£o inteligente de tokens para otimizar rate limits
- âœ… Dados consolidados em data lake (PostgreSQL)
- âœ… Camada analÃ­tica pronta para BI (SQL Server)
- âœ… Agendamento automÃ¡tico (3x por dia)
- âœ… Tratamento robusto de erros e retries
- âœ… EscalÃ¡vel para centenas de contas

### Impacto

- â±ï¸ **Economia de tempo**: Horas de trabalho manual â†’ AutomÃ¡tico
- ğŸ“Š **Qualidade de dados**: Dados consistentes e validados
- ğŸš€ **Escalabilidade**: FÃ¡cil adicionar novas contas
- ğŸ”’ **Confiabilidade**: Retry automÃ¡tico, logging completo
- ğŸ“ˆ **Insights**: Dados prontos para anÃ¡lise e BI

---

**Nota**: Todas as informaÃ§Ãµes sensÃ­veis (credenciais, IDs de contas, nomes de empresas) foram removidas e substituÃ­das por placeholders de variÃ¡veis de ambiente. Isso garante que o cÃ³digo possa ser compartilhado com seguranÃ§a mantendo as melhores prÃ¡ticas de seguranÃ§a.
# meta-ads-airflow-pipeline
